{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76813b50",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Preprocessing Notebook â€“ Tutas Recommender\n",
    "\n",
    "This notebook handles the preprocessing pipeline **after feature engineering in Google Cloud BigQuery**.  \n",
    "The feature-engineered dataset (`tutas_recommender_training_features.csv`) is exported from BigQuery and used here as input.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "- Load the feature-engineered dataset from BigQuery export.\n",
    "- Clean and prepare the data for machine learning.\n",
    "- Split into training and testing sets (`X_train`, `X_test`, `y_train`, `y_test`).\n",
    "- Save processed files under `dataset/processed_2/` for model training.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **Note**:  \n",
    "- The dataset is **synthetic** and was created for demonstration purposes.  \n",
    "- No real user data is involved.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28473b",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "We start by importing the required libraries for preprocessing:\n",
    "\n",
    "- **pandas** â†’ to handle and manipulate tabular datasets.  \n",
    "- **sklearn.preprocessing.StandardScaler** â†’ to standardize numerical features (zero mean, unit variance).  \n",
    "- **sklearn.model_selection.train_test_split** â†’ to split the dataset into training and testing subsets.\n",
    "\n",
    "These libraries will be used throughout the notebook to clean, transform, and prepare the data for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8219b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a2049",
   "metadata": {},
   "source": [
    "## 2. Load the Feature-Engineered Dataset\n",
    "\n",
    "We load the dataset exported from **BigQuery** (`tutas_recommender_training_features.csv`).  \n",
    "This dataset contains feature-engineered records representing studentâ€“tutor interactions.\n",
    "\n",
    "After loading:\n",
    "- Display the first few rows with `head()` to get an overview.  \n",
    "- Use `info()` to check data types and non-null counts.  \n",
    "- Use `isnull().sum()` to identify any missing values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature-engineered dataset exported from BigQuery\n",
    "df = pd.read_csv('../data/dataset/processed_1/tutas_recommender_training_features.csv')\n",
    "\n",
    "# Preview the first 5 rows to understand dataset structure\n",
    "print(df.head())\n",
    "\n",
    "# Display data types, column info, and non-null counts\n",
    "df.info()\n",
    "\n",
    "# Check for missing values in each column\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b7bab",
   "metadata": {},
   "source": [
    "## 3. Identify Numerical Features\n",
    "\n",
    "We select all columns with numeric data types (`int64`, `float64`) for further preprocessing.  \n",
    "This step is important because numerical features will later be **standardized** using `StandardScaler` to improve model performance.\n",
    "\n",
    "After selecting:\n",
    "- Print the list of numerical features.  \n",
    "- Use `describe()` to view basic statistics (mean, std, min, max, quartiles) for these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912620bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with numeric data types (int64, float64)\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Print the list of numerical feature names\n",
    "print(\"Fitur Numerikal : \", list(numerical_cols))\n",
    "\n",
    "# Show descriptive statistics for numerical features\n",
    "print(df[numerical_cols].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86393d",
   "metadata": {},
   "source": [
    "## 4. Encode Boolean Features\n",
    "\n",
    "Some columns contain boolean values (`True` / `False`).  \n",
    "Since most machine learning algorithms expect numeric inputs, we convert these boolean features into integers:\n",
    "- `True` â†’ 1  \n",
    "- `False` â†’ 0  \n",
    "\n",
    "This ensures all features are numeric and ready for preprocessing or scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all boolean columns into integer values (True=1, False=0)\n",
    "for col in df.select_dtypes(include=['bool']).columns:\n",
    "    df[col] = df[col].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc58d8e",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling\n",
    "\n",
    "We apply **StandardScaler** to normalize numerical features.  \n",
    "- StandardScaler standardizes features by removing the mean and scaling to unit variance.  \n",
    "- It works well because the data distribution is approximately normal and does not contain many outliers.  \n",
    "- If outliers were present, a **RobustScaler** would be more appropriate.\n",
    "\n",
    "Steps:\n",
    "1. Separate features (`X`) and target (`y`).  \n",
    "2. Select numerical columns to be scaled.  \n",
    "3. Apply `StandardScaler` only to the selected numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target label (y)\n",
    "X = df.drop(columns=['label', 'id_murid', 'id_tutor'])  # remove label and ID columns\n",
    "y = df['label']\n",
    "\n",
    "# Define numerical columns to be standardized\n",
    "numerical_cols = ['feedback_score', 'average_rating_tutor', 'total_jam_ngajar_tutor']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling to the selected numerical features\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173bcb6",
   "metadata": {},
   "source": [
    "## 6. Trainâ€“Test Split\n",
    "\n",
    "We now split the dataset into **training** and **testing** subsets:\n",
    "\n",
    "- **Training set (80%)** â†’ used to train the machine learning model.  \n",
    "- **Testing set (20%)** â†’ held out for evaluation, to check generalization performance.  \n",
    "\n",
    "After splitting, we save each subset as CSV files under `dataset/processed_2/`:\n",
    "- `X_train.csv`, `X_test.csv`\n",
    "- `y_train.csv`, `y_test.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Save the split datasets into processed 2 folder\n",
    "X_train.to_csv('../data/dataset/processed 2/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/dataset/processed 2/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/dataset/processed 2/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/dataset/processed 2/y_test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
