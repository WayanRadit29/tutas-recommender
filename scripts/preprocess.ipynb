{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76813b50",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Preprocessing Notebook â€“ Tutas Recommender\n",
    "\n",
    "This notebook handles the preprocessing pipeline **after feature engineering in Google Cloud BigQuery**.  \n",
    "The feature-engineered dataset (`tutas_recommender_training_features.csv`) is exported from BigQuery and used here as input.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "- Load the feature-engineered dataset from BigQuery export.\n",
    "- Clean and prepare the data for machine learning.\n",
    "- Split into training and testing sets (`X_train`, `X_test`, `y_train`, `y_test`).\n",
    "- Save processed files under `dataset/processed_2/` for model training.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Workflow\n",
    "1. **Import dependencies** â†’ pandas, numpy, sklearn.  \n",
    "2. **Load dataset** â†’ `tutas_recommender_training_features.csv` from `processed_1/`.  \n",
    "3. **Feature/label separation** â†’ split predictors (`X`) and target label (`y`).  \n",
    "4. **Train-test split** â†’ create `X_train`, `X_test`, `y_train`, `y_test`.  \n",
    "5. **Save processed files** â†’ store them in `dataset/processed_2/`.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **Note**:  \n",
    "- The dataset is **synthetic** and was created for demonstration purposes.  \n",
    "- No real user data is involved.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28473b",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "We start by importing the required libraries for preprocessing:\n",
    "\n",
    "- **pandas** â†’ to handle and manipulate tabular datasets.  \n",
    "- **sklearn.preprocessing.StandardScaler** â†’ to standardize numerical features (zero mean, unit variance).  \n",
    "- **sklearn.model_selection.train_test_split** â†’ to split the dataset into training and testing subsets.\n",
    "\n",
    "These libraries will be used throughout the notebook to clean, transform, and prepare the data for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8219b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a2049",
   "metadata": {},
   "source": [
    "## 2. Load the Feature-Engineered Dataset\n",
    "\n",
    "We load the dataset exported from **BigQuery** (`tutas_recommender_training_features.csv`).  \n",
    "This dataset contains feature-engineered records representing studentâ€“tutor interactions.\n",
    "\n",
    "After loading:\n",
    "- Display the first few rows with `head()` to get an overview.  \n",
    "- Use `info()` to check data types and non-null counts.  \n",
    "- Use `isnull().sum()` to identify any missing values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id_murid id_tutor  topik_match  gaya_match  metode_match  time_overlap  \\\n",
      "0    M0566    T0008         True        True          True         False   \n",
      "1    M0379    T0013         True        True          True         False   \n",
      "2    M0964    T0013         True        True          True         False   \n",
      "3    M0100    T0017         True        True          True         False   \n",
      "4    M0416    T0018         True        True          True         False   \n",
      "\n",
      "   murid_flexible  tutor_flexible  feedback_score  label  \\\n",
      "0           False           False               0      0   \n",
      "1           False           False               0      0   \n",
      "2           False           False               0      0   \n",
      "3           False           False               0      0   \n",
      "4           False           False               0      0   \n",
      "\n",
      "   average_rating_tutor  total_jam_ngajar_tutor  pernah_gagal  \n",
      "0                  3.71                      72          True  \n",
      "1                  4.60                     323          True  \n",
      "2                  4.60                     323          True  \n",
      "3                  3.99                     131          True  \n",
      "4                  3.97                      18          True  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2012 entries, 0 to 2011\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id_murid                2012 non-null   object \n",
      " 1   id_tutor                2012 non-null   object \n",
      " 2   topik_match             2012 non-null   bool   \n",
      " 3   gaya_match              2012 non-null   bool   \n",
      " 4   metode_match            2012 non-null   bool   \n",
      " 5   time_overlap            2012 non-null   bool   \n",
      " 6   murid_flexible          2012 non-null   bool   \n",
      " 7   tutor_flexible          2012 non-null   bool   \n",
      " 8   feedback_score          2012 non-null   int64  \n",
      " 9   label                   2012 non-null   int64  \n",
      " 10  average_rating_tutor    2012 non-null   float64\n",
      " 11  total_jam_ngajar_tutor  2012 non-null   int64  \n",
      " 12  pernah_gagal            2012 non-null   bool   \n",
      "dtypes: bool(7), float64(1), int64(3), object(2)\n",
      "memory usage: 108.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id_murid                  0\n",
       "id_tutor                  0\n",
       "topik_match               0\n",
       "gaya_match                0\n",
       "metode_match              0\n",
       "time_overlap              0\n",
       "murid_flexible            0\n",
       "tutor_flexible            0\n",
       "feedback_score            0\n",
       "label                     0\n",
       "average_rating_tutor      0\n",
       "total_jam_ngajar_tutor    0\n",
       "pernah_gagal              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the feature-engineered dataset exported from BigQuery\n",
    "df = pd.read_csv('../data/dataset/processed_1/tutas_recommender_training_features.csv')\n",
    "\n",
    "# Preview the first 5 rows to understand dataset structure\n",
    "print(df.head())\n",
    "\n",
    "# Display data types, column info, and non-null counts\n",
    "df.info()\n",
    "\n",
    "# Check for missing values in each column\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b7bab",
   "metadata": {},
   "source": [
    "## 3. Identify Numerical Features\n",
    "\n",
    "We select all columns with numeric data types (`int64`, `float64`) for further preprocessing.  \n",
    "This step is important because numerical features will later be **standardized** using `StandardScaler` to improve model performance.\n",
    "\n",
    "After selecting:\n",
    "- Print the list of numerical features.  \n",
    "- Use `describe()` to view basic statistics (mean, std, min, max, quartiles) for these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "912620bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur Numerikal :  ['feedback_score', 'label', 'average_rating_tutor', 'total_jam_ngajar_tutor']\n",
      "       feedback_score        label  average_rating_tutor  \\\n",
      "count     2012.000000  2012.000000           2012.000000   \n",
      "mean         3.519384     0.736581              3.973335   \n",
      "std          1.920116     0.440597              0.574820   \n",
      "min          0.000000     0.000000              3.000000   \n",
      "25%          3.000000     0.000000              3.490000   \n",
      "50%          4.000000     1.000000              3.960000   \n",
      "75%          5.000000     1.000000              4.452500   \n",
      "max          5.000000     1.000000              5.000000   \n",
      "\n",
      "       total_jam_ngajar_tutor  \n",
      "count             2012.000000  \n",
      "mean               196.456759  \n",
      "std                109.466772  \n",
      "min                 11.000000  \n",
      "25%                106.000000  \n",
      "50%                193.000000  \n",
      "75%                287.000000  \n",
      "max                400.000000  \n"
     ]
    }
   ],
   "source": [
    "# Select columns with numeric data types (int64, float64)\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Print the list of numerical feature names\n",
    "print(\"Fitur Numerikal : \", list(numerical_cols))\n",
    "\n",
    "# Show descriptive statistics for numerical features\n",
    "print(df[numerical_cols].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86393d",
   "metadata": {},
   "source": [
    "## 4. Encode Boolean Features\n",
    "\n",
    "Some columns contain boolean values (`True` / `False`).  \n",
    "Since most machine learning algorithms expect numeric inputs, we convert these boolean features into integers:\n",
    "- `True` â†’ 1  \n",
    "- `False` â†’ 0  \n",
    "\n",
    "This ensures all features are numeric and ready for preprocessing or scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all boolean columns into integer values (True=1, False=0)\n",
    "for col in df.select_dtypes(include=['bool']).columns:\n",
    "    df[col] = df[col].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc58d8e",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling\n",
    "\n",
    "We apply **StandardScaler** to normalize numerical features.  \n",
    "- StandardScaler standardizes features by removing the mean and scaling to unit variance.  \n",
    "- It works well because the data distribution is approximately normal and does not contain many outliers.  \n",
    "- If outliers were present, a **RobustScaler** would be more appropriate.\n",
    "\n",
    "Steps:\n",
    "1. Separate features (`X`) and target (`y`).  \n",
    "2. Select numerical columns to be scaled.  \n",
    "3. Apply `StandardScaler` only to the selected numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target label (y)\n",
    "X = df.drop(columns=['label', 'id_murid', 'id_tutor'])  # remove label and ID columns\n",
    "y = df['label']\n",
    "\n",
    "# Define numerical columns to be standardized\n",
    "numerical_cols = ['feedback_score', 'average_rating_tutor', 'total_jam_ngajar_tutor']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling to the selected numerical features\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173bcb6",
   "metadata": {},
   "source": [
    "## 6. Trainâ€“Test Split\n",
    "\n",
    "We now split the dataset into **training** and **testing** subsets:\n",
    "\n",
    "- **Training set (80%)** â†’ used to train the machine learning model.  \n",
    "- **Testing set (20%)** â†’ held out for evaluation, to check generalization performance.  \n",
    "\n",
    "After splitting, we save each subset as CSV files under `dataset/processed_2/`:\n",
    "- `X_train.csv`, `X_test.csv`\n",
    "- `y_train.csv`, `y_test.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1f593",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\data\\dataset\\processed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Sekarang kita split untuk data training dan testing. \u001b[39;00m\n\u001b[32m      3\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/dataset/processed/X_train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m X_test.to_csv(\u001b[33m'\u001b[39m\u001b[33m../data/dataset/processed/X_test.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m y_train.to_csv(\u001b[33m'\u001b[39m\u001b[33m../data/dataset/processed/y_train.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine_Learning_Journey\\ML_venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine_Learning_Journey\\ML_venv\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine_Learning_Journey\\ML_venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine_Learning_Journey\\ML_venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine_Learning_Journey\\ML_venv\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine_Learning_Journey\\ML_venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '..\\data\\dataset\\processed'"
     ]
    }
   ],
   "source": [
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Save the split datasets into processed 2 folder\n",
    "X_train.to_csv('../data/dataset/processed 2/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/dataset/processed 2/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/dataset/processed 2/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/dataset/processed 2/y_test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
