# ğŸ¯ Tutas Recommender â€“ AI Tutor Matching System

This project builds a machine learning-based tutor-student recommendation system using synthetic data and Google Cloud Platform tools.

---

## ğŸš€ Project Goals
- Build an AI recommendation system to match tutors and students based on preferences and feedback
- Train ML models using synthetic data
- Implement an end-to-end ML workflow with BigQuery, SQL, and Vertex AI AutoML
- Serve as a personal portfolio to demonstrate ML & GCP proficiency

---
## ğŸ“‚ Project Structure

.
â”œâ”€â”€ data/                           # Data generation & preprocessing
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ unprocessed/             # Raw synthetic data (output of main.py)
â”‚   â”‚   â”‚   â”œâ”€â”€ murid.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ tutor.csv
â”‚   â”‚   â”‚   â””â”€â”€ interaksi.csv
â”‚   â”‚   â”œâ”€â”€ processed 1/             # BigQuery engineered dataset
â”‚   â”‚   â”‚   â””â”€â”€ tutas\_recommender\_training\_features.csv
â”‚   â”‚   â””â”€â”€ processed 2/             # Preprocessed splits for ML model
â”‚   â”‚       â”œâ”€â”€ X\_train.csv
â”‚   â”‚       â”œâ”€â”€ X\_test.csv
â”‚   â”‚       â”œâ”€â”€ y\_train.csv
â”‚   â”‚       â””â”€â”€ y\_test.csv
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ generators.py                # Synthetic data generators (student, tutor, interactions)
â”‚   â”œâ”€â”€ main.py                      # Entry point to generate unprocessed data
â”‚   â”œâ”€â”€ utils.py                     # Helper functions
â”‚   â””â”€â”€ readme.md                    # Detailed data processing documentation
â”‚
â”œâ”€â”€ docs/                            # Documentation & figures
â”‚   â”œâ”€â”€ processing\_data\_in\_BigQuery/ # SQL scripts & explanations
â”‚   â”‚   â”œâ”€â”€ bigquery\_sql/
â”‚   â”‚   â”‚   â”œâ”€â”€ 01\_validate\_distribution.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ 02\_pernah\_gagal\_features.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ 03\_training\_features\_join.sql
â”‚   â”‚   â”‚   â””â”€â”€ 04\_insight\_pairing\_failures.sql
â”‚   â”‚   â”œâ”€â”€ pictures/                # Supporting figures
â”‚   â”‚   â”‚   â”œâ”€â”€ create\_bucket\_and\_upload\_dataset.png
â”‚   â”‚   â”‚   â”œâ”€â”€ create\_table.png
â”‚   â”‚   â”‚   â”œâ”€â”€ label\_distribution.png
â”‚   â”‚   â”‚   â”œâ”€â”€ feedback\_score\_distribution.png
â”‚   â”‚   â”‚   â”œâ”€â”€ anomali\_label\_check.png
â”‚   â”‚   â”‚   â”œâ”€â”€ create\_pernah\_gagal\_features.png
â”‚   â”‚   â”‚   â””â”€â”€ make\_tutas\_training\_dataset.png
â”‚   â”‚   â””â”€â”€ readme.md
â”‚   â”œâ”€â”€ training\_model\_in\_VertexAI/  # Vertex AI training setup & screenshots
â”‚   â”œâ”€â”€ deploying\_model/             # Deployment process & screenshots
â”‚   â””â”€â”€ Inference\_test\_and\_Evaluation/ # Inference & evaluation figures
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ preprocess.ipynb             # Notebook for scaling & splitting data
â”‚
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train.py                     # Model training script (Keras/TensorFlow)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ evaluation/                  # Evaluation scripts
â”‚   â”‚   â””â”€â”€ evaluation.py
â”‚   â”œâ”€â”€ logs/                        # TensorBoard logs
â”‚   â””â”€â”€ models/                      # Saved model checkpoints
â”‚       â””â”€â”€ tutas-v1/
â”‚           â”œâ”€â”€ saved\_model.pb
â”‚           â””â”€â”€ variables/
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ readme.md                        # Main project documentation
---

## ğŸ§± Tech Stack
- Python (for data generation)
- Google BigQuery (data warehousing & feature engineering)
- Vertex AI AutoML Tabular (model training)
- GitHub & Notion (documentation & presentation)
- Libraries: Faker, Pandas, Jupyter

---

## ğŸ“Š Dataset Overview

### ğŸ“ Datasets
- `murid_data`: Student preferences, schedule, learning style
- `tutor_data`: Tutor expertise, ratings, schedule, experience
- `interaksi_data`: Match label, feedback score, joined feature set

### ğŸ§  Training Features
- `topik_match`, `gaya_match`, `metode_match`, `time_overlap`,  
  `murid_flexible`, `tutor_flexible`, `pernah_gagal`, `rating`, `total_jam_ngajar`

---

## ğŸ”„ ML Workflow (on GCP)
1. Generate synthetic data using Python
2. Upload datasets to BigQuery
3. Feature engineering with SQL (`pernah_gagal`, join tutor info)
4. Train AutoML model using Vertex AI
5. Evaluate & (optionally) deploy the model

---

## ğŸ“ˆ Results & Evaluation
_(To be completed after AutoML training â€” include screenshots of metrics)_

---

## âœï¸ Author
Wayan Raditya Putra â€“ AI Engineering student at ITS  
This project was built as a personal branding portfolio to showcase GCP and ML skills.
