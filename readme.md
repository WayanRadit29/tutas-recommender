# ğŸ¯ Tutas Recommender â€“ AI Tutor Matching System

This project builds a machine learning-based tutor-student recommendation system using synthetic data and Google Cloud Platform tools.

---

## ğŸš€ Project Goals
- Build an AI recommendation system to match tutors and students based on preferences and feedback
- Train ML models using synthetic data
- Implement an end-to-end ML workflow with BigQuery, SQL, and Vertex AI AutoML
- Serve as a personal portfolio to demonstrate ML & GCP proficiency

---
## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ data/                           # Data generation & preprocessing
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ unprocessed/             # Raw synthetic data (output of main.py)
â”‚   â”‚   â”‚   â”œâ”€â”€ murid.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ tutor.csv
â”‚   â”‚   â”‚   â””â”€â”€ interaksi.csv
â”‚   â”‚   â”œâ”€â”€ processed 1/             # BigQuery engineered dataset
â”‚   â”‚   â”‚   â””â”€â”€ tutas_recommender_training_features.csv
â”‚   â”‚   â””â”€â”€ processed 2/             # Preprocessed splits for ML model
â”‚   â”‚       â”œâ”€â”€ X_train.csv
â”‚   â”‚       â”œâ”€â”€ X_test.csv
â”‚   â”‚       â”œâ”€â”€ y_train.csv
â”‚   â”‚       â””â”€â”€ y_test.csv
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ generators.py                # Synthetic data generators (student, tutor, interactions)
â”‚   â”œâ”€â”€ main.py                      # Entry point to generate unprocessed data
â”‚   â”œâ”€â”€ utils.py                     # Helper functions
â”‚   â””â”€â”€ readme.md                    # Detailed data processing documentation
â”‚
â”œâ”€â”€ docs/                            # Documentation & figures
â”‚   â”œâ”€â”€ processing_data_in_BigQuery/ # SQL scripts & explanations
â”‚   â”‚   â”œâ”€â”€ bigquery_sql/
â”‚   â”‚   â”‚   â”œâ”€â”€ 01_validate_distribution.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ 02_pernah_gagal_features.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ 03_training_features_join.sql
â”‚   â”‚   â”‚   â””â”€â”€ 04_insight_pairing_failures.sql
â”‚   â”‚   â”œâ”€â”€ pictures/                # Supporting figures
â”‚   â”‚   â”‚   â”œâ”€â”€ create_bucket_and_upload_dataset.png
â”‚   â”‚   â”‚   â”œâ”€â”€ create_table.png
â”‚   â”‚   â”‚   â”œâ”€â”€ label_distribution.png
â”‚   â”‚   â”‚   â”œâ”€â”€ feedback_score_distribution.png
â”‚   â”‚   â”‚   â”œâ”€â”€ anomali_label_check.png
â”‚   â”‚   â”‚   â”œâ”€â”€ create_pernah_gagal_features.png
â”‚   â”‚   â”‚   â””â”€â”€ make_tutas_training_dataset.png
â”‚   â”‚   â””â”€â”€ readme.md
â”‚   â”œâ”€â”€ training_model_in_VertexAI/
â”‚   â”œâ”€â”€ deploying_model/
â”‚   â””â”€â”€ Inference_test_and_Evaluation/
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ preprocess.ipynb
â”‚
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ evaluation.py
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ tutas-v1/
â”‚           â”œâ”€â”€ saved_model.pb
â”‚           â””â”€â”€ variables/
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ readme.md                        # Main project documentation

```

---

## ğŸ“š Detailed Documentation

For a full walkthrough of **data preprocessing** and **feature engineering** (synthetic data â†’ BigQuery â†’ processed datasets), check the dedicated documentation here:

ğŸ‘‰ [Data Processing & Feature Engineering](data/readme.md)
ğŸ‘‰ [Training Model in Vertex AI](training/readme.md)
ğŸ‘‰ [Deploying model in Vertex AI](outputs/Readme.md)
ğŸ‘‰ [Inference and Evaluation of Model](outputs//evaluation/readme.md)

---

## ğŸ§± Tech Stack
- Python (for data generation)
- Google BigQuery (data warehousing & feature engineering)
- Vertex AI AutoML Tabular (model training)
- GitHub & Notion (documentation & presentation)
- Libraries: Faker, Pandas, Jupyter

---
## ğŸš€ Usage

Hereâ€™s the end-to-end workflow to run the **Tutas Recommender â€“ AI Tutor Matching System**:

1. **ğŸ“¦ Generate Synthetic Data**
   ```bash
   python main.py
   ```

This creates raw synthetic datasets (`murid.csv`, `tutor.csv`, `interaksi.csv`) inside [`data/dataset/unprocessed/`](data/dataset/unprocessed/).

2. **ğŸ—„ï¸ Data Processing & Feature Engineering**

   * Upload datasets to **Google BigQuery**.
   * Run SQL scripts to validate, join, and create features like `pernah_gagal`.
   * See detailed steps in:
     ğŸ‘‰ [`data/readme.md`](data/readme.md)
     ğŸ‘‰ [`docs/processing_data_in_BigQuery/readme.md`](docs/processing_data_in_BigQuery/readme.md)

3. **âš™ï¸ Preprocessing for ML**

   * Split into train/test sets.
   * Outputs stored in [`data/dataset/processed 2/`](data/dataset/processed%202/).
     Example:

     ```
     X_train.csv | X_test.csv | y_train.csv | y_test.csv
     ```

4. **ğŸ¤– Train the Model (Vertex AI Workbench)**

   ```bash
   python training/train.py
   ```

   * Uses TensorFlow backend.
   * Logs and checkpoints saved in `outputs/logs/`.
   * Trained model exported to `models/tutas-v1/`.

5. **ğŸŒ Deploy & Inference (Vertex AI)**

   * Deploy model to an endpoint via **Vertex AI**.
   * Test with JSON requests:

     ```json
     {
       "instances": [[1, 1, 1, 0, 1, 0, 0.771300, 0.672839, -1.018432, 0]]
     }
     ```
   * Deployment steps: ğŸ‘‰ [`docs/deploying_model/`](docs/deploying_model/)

6. **ğŸ“ˆ Evaluate the Model**

   ```bash
   python outputs/evaluation/evaluation.py
   ```

   * Generates classification report & confusion matrix.
   * See details: ğŸ‘‰ [`docs/Inference_test_and_Evaluation/`](docs/Inference_test_and_Evaluation/)



---

## ğŸ”„ ML Workflow (on GCP)
1. Generate synthetic data using Python
2. Upload datasets to BigQuery
3. Feature engineering with SQL (`pernah_gagal`, join tutor info)
4. Train AutoML model using Vertex AI
5. Evaluate & (optionally) deploy the model
---

## âœï¸ Author
Wayan Raditya Putra â€“ AI Engineering student at ITS  
This project was built as a personal branding portfolio to showcase GCP and ML skills.
